{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6218efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a207689",
   "metadata": {},
   "source": [
    "## 1.å®šä¹‰ç®—æ³•\n",
    "### 1.1 å»ºç«‹Qç½‘ç»œå’Œç­–ç•¥ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5955151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, n_states, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNet, self).__init__()\n",
    "        '''å®šä¹‰å€¼ç½‘ç»œ\n",
    "        '''\n",
    "        self.linear1 = nn.Linear(n_states, hidden_dim) # è¾“å…¥å±‚\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # éšè—å±‚\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w) # åˆå§‹åŒ–æƒé‡\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class SoftQNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim, init_w=3e-3):\n",
    "        super(SoftQNet, self).__init__()\n",
    "        '''å®šä¹‰Qç½‘ç»œï¼Œn_states, n_actions, hidden_dim, init_wåˆ†åˆ«ä¸ºçŠ¶æ€ç»´åº¦ã€åŠ¨ä½œç»´åº¦éšè—å±‚ç»´åº¦å’Œåˆå§‹åŒ–æƒé‡\n",
    "        '''\n",
    "        self.linear1 = nn.Linear(n_states + n_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        '''å®šä¹‰ç­–ç•¥ç½‘ç»œï¼Œn_states, n_actions, hidden_dim, init_wåˆ†åˆ«ä¸ºçŠ¶æ€ç»´åº¦ã€åŠ¨ä½œç»´åº¦éšè—å±‚ç»´åº¦å’Œåˆå§‹åŒ–æƒé‡\n",
    "        log_std_minå’Œlog_std_maxä¸ºæ ‡å‡†å·®å¯¹æ•°çš„æœ€å¤§å€¼å’Œæœ€å°å€¼é˜²æ­¢æ¢ç´¢ç¨‹åº¦è¿‡å¤§æˆ–è¿‡å°\n",
    "        '''\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(n_states, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # å‡å€¼é¢„æµ‹ç½‘ç»œåˆ†æ”¯\n",
    "        self.mean_linear = nn.Linear(hidden_dim, n_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        # æ ‡å‡†å·®é¢„æµ‹ç½‘ç»œåˆ†æ”¯\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, n_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        ## è®¡ç®—åŠ¨ä½œæ„å»ºæ­£æ€åˆ†å¸ƒ\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "        action = torch.tanh(z)# ä½¿ç”¨tanhå‡½æ•°å°†åŠ¨ä½œé™åˆ¶åœ¨[-1,1]èŒƒå›´å†…\n",
    "        ## è®¡ç®—åŠ¨ä½œæ¦‚ç‡\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z      = normal.sample()\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        action  = action.detach().cpu().numpy()\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede08b5",
   "metadata": {},
   "source": [
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  \n",
    "â”‚                         ç¯å¢ƒ (Environment)                           â”‚  \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \n",
    "                                â”‚  \n",
    "                                â–¼ çŠ¶æ€ s_t \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” \n",
    "â”‚                        ç­–ç•¥ç½‘ç»œ (Policy Network)                      â”‚ \n",
    "â”‚                                                                      â”‚ \n",
    "â”‚   è¾“å…¥: çŠ¶æ€ s_t                                                      â”‚ \n",
    "â”‚   è¾“å‡º: åŠ¨ä½œåˆ†å¸ƒå‚æ•° (å‡å€¼Î¼, å¯¹æ•°æ ‡å‡†å·®log_Ïƒ)                          â”‚\n",
    "â”‚   åŠŸèƒ½: é€šè¿‡é‡å‚æ•°åŒ–é‡‡æ ·ç”ŸæˆåŠ¨ä½œ a_t                                   â”‚ \n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚ \n",
    "                    â”‚ åŠ¨ä½œ a_t\n",
    "                    â–¼ \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          ç¯å¢ƒ (Environment)                          â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚   è¾“å…¥: çŠ¶æ€ s_t, åŠ¨ä½œ a_t                                           â”‚\n",
    "â”‚   è¾“å‡º: å¥–åŠ± r_t, ä¸‹ä¸€çŠ¶æ€ s_{t+1}, ç»ˆæ­¢æ ‡å¿— done                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚\n",
    "            â”‚ (s_t, a_t, r_t, s_{t+1}, done)\n",
    "            â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     ç»éªŒå›æ”¾ç¼“å†²åŒº (Replay Buffer)                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚ \n",
    "                                â”‚ æ‰¹é‡é‡‡æ · (s, a, r, s', done)\n",
    "                                â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           è®­ç»ƒè¿‡ç¨‹ (Update)                           â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚     åŒQç½‘ç»œæ›´æ–°      â”‚  â”‚     å€¼ç½‘ç»œæ›´æ–°      â”‚  â”‚   ç­–ç•¥ç½‘ç»œæ›´æ–°   â”‚ â”‚\n",
    "â”‚  â”‚ (Twin Q Networks)   â”‚  â”‚  (Value Network)   â”‚  â”‚(Policy Network)â”‚ â”‚\n",
    "â”‚  â”‚                     â”‚  â”‚                    â”‚  â”‚                â”‚ â”‚\n",
    "â”‚  â”‚ ç›®æ ‡: æœ€å°åŒ–TDè¯¯å·®    â”‚  â”‚ ç›®æ ‡: æ‹ŸåˆQ - Î±Ã—logâ”‚  â”‚ ç›®æ ‡: æœ€å¤§åŒ–    â”‚ â”‚\n",
    "â”‚  â”‚ L_Q = (Q - (r +     â”‚  â”‚ L_V = (V - (Q -    â”‚  â”‚ J_Ï€ = E[Q -    â”‚ â”‚\n",
    "â”‚  â”‚      Î³V'(s')))Â²     â”‚  â”‚     Î±log Ï€))Â²      â”‚  â”‚    Î±log Ï€]     â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚                  ç›®æ ‡ç½‘ç»œè½¯æ›´æ–° (Soft Update)                     â”‚ â”‚\n",
    "â”‚  â”‚                                                                 â”‚ â”‚\n",
    "â”‚  â”‚  V'(s) â† Ï„V(s) + (1-Ï„)V'(s)                                    â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚                  æ¸©åº¦å‚æ•°æ›´æ–° (Temperature Î±)                     â”‚ â”‚\n",
    "â”‚  â”‚                                                                 â”‚ â”‚\n",
    "â”‚  â”‚  æœ€å°åŒ–: L_Î± = -Î± Ã— log Ï€ - Î± Ã— H_target                        â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9c3c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "debce530",
   "metadata": {},
   "source": [
    "### 1.2 å®šä¹‰ç»éªŒå›æ”¾æ± "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c740ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # ç»éªŒå›æ”¾çš„å®¹é‡\n",
    "        self.buffer = [] # ç¼“å†²åŒº\n",
    "        self.position = 0 \n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        ''' ç¼“å†²åŒºæ˜¯ä¸€ä¸ªé˜Ÿåˆ—ï¼Œå®¹é‡è¶…å‡ºæ—¶å»æ‰å¼€å§‹å­˜å…¥çš„è½¬ç§»(transition)\n",
    "        '''\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size) # éšæœºé‡‡å‡ºå°æ‰¹é‡è½¬ç§»\n",
    "        state, action, reward, next_state, done =  zip(*batch) # è§£å‹æˆçŠ¶æ€ï¼ŒåŠ¨ä½œç­‰\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' è¿”å›å½“å‰å­˜å‚¨çš„é‡\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a65b71",
   "metadata": {},
   "source": [
    "### 1.3 SACç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a86f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self,cfg) -> None:\n",
    "        self.n_states = cfg.n_states\n",
    "        self.n_actions = cfg.n_actions\n",
    "        self.batch_size  = cfg.batch_size \n",
    "        self.memory = ReplayBuffer(cfg.capacity)\n",
    "        self.device = cfg.device\n",
    "        self.action_space = cfg.action_space\n",
    "        self.value_net  = ValueNet(self.n_states, cfg.hidden_dim).to(self.device)\n",
    "        self.target_value_net = ValueNet(self.n_states, cfg.hidden_dim).to(self.device)\n",
    "        self.soft_q_net = SoftQNet(self.n_states, self.n_actions, cfg.hidden_dim).to(self.device)\n",
    "        self.policy_net = PolicyNet(self.n_states, self.n_actions, cfg.hidden_dim).to(self.device)  \n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=cfg.value_lr)\n",
    "        self.soft_q_optimizer = Adam(self.soft_q_net.parameters(), lr=cfg.soft_q_lr)\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=cfg.policy_lr)  \n",
    "        for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        self.value_criterion  = nn.MSELoss()\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "    def update(self, gamma=0.99,mean_lambda=1e-3,\n",
    "        std_lambda=1e-3,\n",
    "        z_lambda=0.0,\n",
    "        soft_tau=1e-2,\n",
    "        ):\n",
    "        if len(self.memory) < self.batch_size: # å½“ç»éªŒå›æ”¾ä¸­ä¸æ»¡è¶³ä¸€ä¸ªæ‰¹é‡æ—¶ï¼Œä¸æ›´æ–°ç­–ç•¥\n",
    "            return \n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size) # ä»ç»éªŒå›æ”¾ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªæ‰¹é‡çš„è½¬ç§»(transition)\n",
    "        # å°†æ•°æ®è½¬æ¢ä¸ºtensor\n",
    "        state      = torch.FloatTensor(state).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        action     = torch.FloatTensor(action).to(self.device)\n",
    "        reward     = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n",
    "        done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        expected_q_value = self.soft_q_net(state, action) #è®¡ç®—tæ—¶åˆ»çš„çŠ¶æ€-åŠ¨ä½œQå€¼\n",
    "        expected_value   = self.value_net(state) #è®¡ç®—tæ—¶åˆ»çš„çŠ¶æ€å€¼\n",
    "        new_action, log_prob, z, mean, log_std = self.policy_net.evaluate(state) #è®¡ç®—tæ—¶åˆ»çš„åŠ¨ä½œã€åŠ¨ä½œä¼¼ç„¶æ¦‚ç‡ã€æ­£æ€åˆ†å¸ƒæŠ½æ ·ã€åˆ†å¸ƒå‡å€¼å’Œæ ‡å‡†å·®\n",
    "\n",
    "\n",
    "        target_value = self.target_value_net(next_state) #è®¡ç®—t+1æ—¶åˆ»çš„çŠ¶æ€å€¼\n",
    "        next_q_value = reward + (1 - done) * gamma * target_value # æ—¶åºå·®åˆ†è®¡ç®—t+1æ—¶åˆ»çš„Qå€¼\n",
    "        # JQ = ğ”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ğ”¼st+1~p[V(st+1)]))^2]\n",
    "        q_value_loss = self.soft_q_criterion(expected_q_value, next_q_value.detach()) #è®¡ç®—qç½‘è·¯çš„æŸå¤±å‡½æ•°\n",
    "\n",
    "        expected_new_q_value = self.soft_q_net(state, new_action) #è®¡ç®—tæ—¶åˆ»åŠ¨ä½œå¯¹åº”çš„qå€¼\n",
    "        next_value = expected_new_q_value - log_prob # è®¡ç®—tæ—¶åˆ»çš„vå€¼\n",
    "        value_loss = self.value_criterion(expected_value, next_value.detach()) #è®¡ç®—å€¼ç½‘ç»œæŸå¤±å‡½æ•°\n",
    "        \n",
    "        ## è®¡ç®—ç­–ç•¥æŸå¤±\n",
    "        log_prob_target = expected_new_q_value - expected_value \n",
    "        # JÏ€ = ğ”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "        policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "\n",
    "        ## è®¡ç®—reparameterizationå‚æ•°æŸå¤±\n",
    "        mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "        std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "        z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "\n",
    "        policy_loss += mean_loss + std_loss + z_loss\n",
    "\n",
    "        self.soft_q_optimizer.zero_grad()\n",
    "        q_value_loss.backward()\n",
    "        self.soft_q_optimizer.step()\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        ## æ›´æ–°ç›®æ ‡å€¼ç½‘ç»œå‚æ•°\n",
    "        for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2581771",
   "metadata": {},
   "source": [
    "## 2.æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3e3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    print(\"å¼€å§‹è®­ç»ƒï¼\")\n",
    "    rewards = [] # è®°å½•æ‰€æœ‰å›åˆçš„å¥–åŠ±\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        state = env.reset() # é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹çŠ¶æ€\n",
    "        ep_reward = 0 # è®°å½•ä¸€å›åˆå†…çš„å¥–åŠ±\n",
    "        for i_step in range(cfg.max_steps):\n",
    "            action = agent.policy_net.get_action(state)  # æŠ½æ ·åŠ¨ä½œ\n",
    "            next_state, reward, terminated, info = env.step(action)  # æ›´æ–°ç¯å¢ƒï¼Œè¿”å›transitions\n",
    "            agent.memory.push(state, action, reward,next_state, terminated)  # ä¿å­˜transition\n",
    "            agent.update()  # æ›´æ–°æ™ºèƒ½ä½“\n",
    "            state = next_state  # æ›´æ–°ä¸‹ä¸€ä¸ªçŠ¶æ€\n",
    "            ep_reward += reward  # ç´¯åŠ å¥–åŠ±\n",
    "            if terminated:\n",
    "                break\n",
    "        if (i_ep+1)%10 == 0:\n",
    "            print(f\"å›åˆï¼š{i_ep+1}/{cfg.train_eps}ï¼Œå¥–åŠ±ï¼š{ep_reward:.2f}\")\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"å®Œæˆè®­ç»ƒï¼\")\n",
    "    return {'rewards':rewards}\n",
    "def test(cfg, env, agent):\n",
    "    print(\"å¼€å§‹æµ‹è¯•ï¼\")\n",
    "    rewards = [] # è®°å½•æ‰€æœ‰å›åˆçš„å¥–åŠ±\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        state = env.reset() # é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹çŠ¶æ€\n",
    "        ep_reward = 0\n",
    "        for i_step in range(cfg.max_steps):\n",
    "            action = agent.policy_net.get_action(state)  # æŠ½æ ·åŠ¨ä½œ\n",
    "            next_state, reward, terminated, info = env.step(action)  # æ›´æ–°ç¯å¢ƒï¼Œè¿”å›transitions\n",
    "            state = next_state  # æ›´æ–°ä¸‹ä¸€ä¸ªçŠ¶æ€\n",
    "            ep_reward += reward  # ç´¯åŠ å¥–åŠ±\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"å›åˆï¼š{i_ep+1}/{cfg.test_eps}ï¼Œå¥–åŠ±ï¼š{ep_reward:.2f}\")\n",
    "    print(\"å®Œæˆæµ‹è¯•ï¼\")\n",
    "    return {'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d45832",
   "metadata": {},
   "source": [
    "## 3.å®šä¹‰ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b94efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        return action\n",
    "    \n",
    "def all_seed(env,seed = 1):\n",
    "    ''' ä¸‡èƒ½çš„seedå‡½æ•°\n",
    "    '''\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "def env_agent_config(cfg):\n",
    "    env = NormalizedActions(gym.make(cfg.env_name)) # åˆ›å»ºç¯å¢ƒ\n",
    "    all_seed(env,seed=cfg.seed)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    print(f\"çŠ¶æ€ç©ºé—´ç»´åº¦ï¼š{n_states}ï¼ŒåŠ¨ä½œç©ºé—´ç»´åº¦ï¼š{n_actions}\")\n",
    "    # æ›´æ–°n_stateså’Œn_actionsåˆ°cfgå‚æ•°ä¸­\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions) \n",
    "    setattr(cfg, 'action_space', env.action_space) \n",
    "    agent = SAC(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87423249",
   "metadata": {},
   "source": [
    "## 4.è®¾ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd710ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.algo_name = 'SAC'\n",
    "        self.env_name = 'Pendulum-v1'\n",
    "        self.seed = 50 # éšæœºç§å­\n",
    "        self.train_eps = 400 # è®­ç»ƒè¿­ä»£æ¬¡æ•°\n",
    "        self.test_eps = 10 # æµ‹è¯•è¿­ä»£æ¬¡æ•°\n",
    "        self.eval_eps = 10 # è¯„ä¼°è¿­ä»£æ¬¡æ•°\n",
    "        self.max_steps = 200 # æ¯æ¬¡è¿­ä»£æœ€å¤§æ—¶é—´æ­¥\n",
    "        self.gamma = 0.99 #æŠ˜æ‰£å› å­\n",
    "        self.mean_lambda=1e-3 # é‡å‚æ•°åŒ–åˆ†å¸ƒå‡å€¼çš„æŸå¤±æƒé‡\n",
    "        self.std_lambda=1e-3 # é‡å‚æ•°åŒ–åˆ†å¸ƒæ ‡å‡†å·®çš„æŸå¤±æƒé‡\n",
    "        self.z_lambda=0.0 # é‡å‚æ•°åŒ–åˆ†å¸ƒæŠ½æ ·å€¼çš„æŸå¤±æƒé‡\n",
    "        self.soft_tau=1e-2 # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°ç³»æ•°\n",
    "        self.value_lr  = 3e-4 # å€¼ç½‘ç»œçš„å­¦ä¹ ç‡\n",
    "        self.soft_q_lr = 3e-4 # Qç½‘ç»œçš„å­¦ä¹ ç‡\n",
    "        self.policy_lr = 3e-4 # ç­–ç•¥ç½‘ç»œçš„å­¦ä¹ ç‡\n",
    "        self.capacity = 1000000 # ç»éªŒç¼“å­˜æ± çš„å¤§å°\n",
    "        self.hidden_dim = 256 # éšè—å±‚ç»´åº¦\n",
    "        self.batch_size  = 128 # æ‰¹æ¬¡å¤§å°\n",
    "        self.start_steps = 1000 # åˆ©ç”¨å‰çš„æ¢ç´¢æ­¥æ•°\n",
    "        self.buffer_size = 1000000 # ç»éªŒå›æ”¾æ± å¤§å°\n",
    "        self.device=torch.device(\"cpu\") # ä½¿ç”¨è®¾å¤‡\n",
    "\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''ç”¨äºå¹³æ»‘æ›²çº¿ï¼Œç±»ä¼¼äºTensorboardä¸­çš„smoothæ›²çº¿\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # è®¡ç®—å¹³æ»‘å€¼\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,title=\"learning curve\"):\n",
    "    sns.set()\n",
    "    plt.figure()  # åˆ›å»ºä¸€ä¸ªå›¾å½¢å®ä¾‹ï¼Œæ–¹ä¾¿åŒæ—¶å¤šç”»å‡ ä¸ªå›¾\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlim(0, len(rewards), 10)  # è®¾ç½®xè½´çš„èŒƒå›´\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3f1c6",
   "metadata": {},
   "source": [
    "## 5.å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a531f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80de3242",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PendulumEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m cfg = Config() \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# è®­ç»ƒ\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m env, agent = \u001b[43menv_agent_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m res_dic = train(cfg, env, agent)\n\u001b[32m      7\u001b[39m plot_rewards(res_dic[\u001b[33m'\u001b[39m\u001b[33mrewards\u001b[39m\u001b[33m'\u001b[39m], title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining curve on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.algo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.env_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36menv_agent_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34menv_agent_config\u001b[39m(cfg):\n\u001b[32m     36\u001b[39m     env = NormalizedActions(gym.make(cfg.env_name)) \u001b[38;5;66;03m# åˆ›å»ºç¯å¢ƒ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mall_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     n_states = env.observation_space.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     39\u001b[39m     n_actions = env.action_space.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mall_seed\u001b[39m\u001b[34m(env, seed)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mall_seed\u001b[39m(env,seed = \u001b[32m1\u001b[39m):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m''' ä¸‡èƒ½çš„seedå‡½æ•°\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m(seed) \u001b[38;5;66;03m# env config\u001b[39;00m\n\u001b[32m     26\u001b[39m     np.random.seed(seed)\n\u001b[32m     27\u001b[39m     random.seed(seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:241\u001b[39m, in \u001b[36mWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maccessing private attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is prohibited\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:241\u001b[39m, in \u001b[36mWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maccessing private attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is prohibited\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Wrapper.__getattr__ at line 241 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:241\u001b[39m, in \u001b[36mWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maccessing private attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is prohibited\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PendulumEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "# è·å–å‚æ•°\n",
    "cfg = Config() \n",
    "# è®­ç»ƒ\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)\n",
    " \n",
    "plot_rewards(res_dic['rewards'], title=f\"training curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  \n",
    "# æµ‹è¯•\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], title=f\"testing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  # ç”»å‡ºç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46214798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badea21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
