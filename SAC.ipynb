{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6218efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a207689",
   "metadata": {},
   "source": [
    "## 1.定义算法\n",
    "### 1.1 建立Q网络和策略网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5955151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, n_states, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNet, self).__init__()\n",
    "        '''定义值网络\n",
    "        '''\n",
    "        self.linear1 = nn.Linear(n_states, hidden_dim) # 输入层\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # 隐藏层\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w) # 初始化权重\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class SoftQNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim, init_w=3e-3):\n",
    "        super(SoftQNet, self).__init__()\n",
    "        '''定义Q网络，n_states, n_actions, hidden_dim, init_w分别为状态维度、动作维度隐藏层维度和初始化权重\n",
    "        '''\n",
    "        self.linear1 = nn.Linear(n_states + n_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        '''定义策略网络，n_states, n_actions, hidden_dim, init_w分别为状态维度、动作维度隐藏层维度和初始化权重\n",
    "        log_std_min和log_std_max为标准差对数的最大值和最小值防止探索程度过大或过小\n",
    "        '''\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(n_states, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # 均值预测网络分支\n",
    "        self.mean_linear = nn.Linear(hidden_dim, n_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        # 标准差预测网络分支\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, n_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        ## 计算动作构建正态分布\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "        action = torch.tanh(z)# 使用tanh函数将动作限制在[-1,1]范围内\n",
    "        ## 计算动作概率\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        z      = normal.sample()\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        action  = action.detach().cpu().numpy()\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede08b5",
   "metadata": {},
   "source": [
    "┌──────────────────────────────────────────────────────────────────────┐  \n",
    "│                         环境 (Environment)                           │  \n",
    "└───────────────────────────────┬──────────────────────────────────────┘  \n",
    "                                │  \n",
    "                                ▼ 状态 s_t \n",
    "┌──────────────────────────────────────────────────────────────────────┐ \n",
    "│                        策略网络 (Policy Network)                      │ \n",
    "│                                                                      │ \n",
    "│   输入: 状态 s_t                                                      │ \n",
    "│   输出: 动作分布参数 (均值μ, 对数标准差log_σ)                          │\n",
    "│   功能: 通过重参数化采样生成动作 a_t                                   │ \n",
    "└───────────────────┬──────────────────────────────────────────────────┘\n",
    "                    │ \n",
    "                    │ 动作 a_t\n",
    "                    ▼ \n",
    "┌──────────────────────────────────────────────────────────────────────┐\n",
    "│                          环境 (Environment)                          │\n",
    "│                                                                      │\n",
    "│   输入: 状态 s_t, 动作 a_t                                           │\n",
    "│   输出: 奖励 r_t, 下一状态 s_{t+1}, 终止标志 done                     │\n",
    "└───────────┬───────────────────────────────────────────────────────────┘\n",
    "            │\n",
    "            │ (s_t, a_t, r_t, s_{t+1}, done)\n",
    "            ▼\n",
    "┌──────────────────────────────────────────────────────────────────────┐\n",
    "│                     经验回放缓冲区 (Replay Buffer)                    │\n",
    "└───────────────────────────────┬──────────────────────────────────────┘\n",
    "                                │ \n",
    "                                │ 批量采样 (s, a, r, s', done)\n",
    "                                ▼\n",
    "┌──────────────────────────────────────────────────────────────────────┐\n",
    "│                           训练过程 (Update)                           │\n",
    "│                                                                      │\n",
    "│  ┌─────────────────────┐  ┌────────────────────┐  ┌────────────────┐ │\n",
    "│  │     双Q网络更新      │  │     值网络更新      │  │   策略网络更新   │ │\n",
    "│  │ (Twin Q Networks)   │  │  (Value Network)   │  │(Policy Network)│ │\n",
    "│  │                     │  │                    │  │                │ │\n",
    "│  │ 目标: 最小化TD误差    │  │ 目标: 拟合Q - α×log│  │ 目标: 最大化    │ │\n",
    "│  │ L_Q = (Q - (r +     │  │ L_V = (V - (Q -    │  │ J_π = E[Q -    │ │\n",
    "│  │      γV'(s')))²     │  │     αlog π))²      │  │    αlog π]     │ │\n",
    "│  └─────────────────────┘  └────────────────────┘  └────────────────┘ │\n",
    "│                                                                      │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐ │\n",
    "│  │                  目标网络软更新 (Soft Update)                     │ │\n",
    "│  │                                                                 │ │\n",
    "│  │  V'(s) ← τV(s) + (1-τ)V'(s)                                    │ │\n",
    "│  └─────────────────────────────────────────────────────────────────┘ │\n",
    "│                                                                      │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐ │\n",
    "│  │                  温度参数更新 (Temperature α)                     │ │\n",
    "│  │                                                                 │ │\n",
    "│  │  最小化: L_α = -α × log π - α × H_target                        │ │\n",
    "│  └─────────────────────────────────────────────────────────────────┘ │\n",
    "└──────────────────────────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9c3c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "debce530",
   "metadata": {},
   "source": [
    "### 1.2 定义经验回放池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c740ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # 经验回放的容量\n",
    "        self.buffer = [] # 缓冲区\n",
    "        self.position = 0 \n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        ''' 缓冲区是一个队列，容量超出时去掉开始存入的转移(transition)\n",
    "        '''\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size) # 随机采出小批量转移\n",
    "        state, action, reward, next_state, done =  zip(*batch) # 解压成状态，动作等\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' 返回当前存储的量\n",
    "        '''\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a65b71",
   "metadata": {},
   "source": [
    "### 1.3 SAC算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a86f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self,cfg) -> None:\n",
    "        self.n_states = cfg.n_states\n",
    "        self.n_actions = cfg.n_actions\n",
    "        self.batch_size  = cfg.batch_size \n",
    "        self.memory = ReplayBuffer(cfg.capacity)\n",
    "        self.device = cfg.device\n",
    "        self.action_space = cfg.action_space\n",
    "        self.value_net  = ValueNet(self.n_states, cfg.hidden_dim).to(self.device)\n",
    "        self.target_value_net = ValueNet(self.n_states, cfg.hidden_dim).to(self.device)\n",
    "        self.soft_q_net = SoftQNet(self.n_states, self.n_actions, cfg.hidden_dim).to(self.device)\n",
    "        self.policy_net = PolicyNet(self.n_states, self.n_actions, cfg.hidden_dim).to(self.device)  \n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=cfg.value_lr)\n",
    "        self.soft_q_optimizer = Adam(self.soft_q_net.parameters(), lr=cfg.soft_q_lr)\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=cfg.policy_lr)  \n",
    "        for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        self.value_criterion  = nn.MSELoss()\n",
    "        self.soft_q_criterion = nn.MSELoss()\n",
    "    def update(self, gamma=0.99,mean_lambda=1e-3,\n",
    "        std_lambda=1e-3,\n",
    "        z_lambda=0.0,\n",
    "        soft_tau=1e-2,\n",
    "        ):\n",
    "        if len(self.memory) < self.batch_size: # 当经验回放中不满足一个批量时，不更新策略\n",
    "            return \n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size) # 从经验回放中随机采样一个批量的转移(transition)\n",
    "        # 将数据转换为tensor\n",
    "        state      = torch.FloatTensor(state).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        action     = torch.FloatTensor(action).to(self.device)\n",
    "        reward     = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n",
    "        done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        expected_q_value = self.soft_q_net(state, action) #计算t时刻的状态-动作Q值\n",
    "        expected_value   = self.value_net(state) #计算t时刻的状态值\n",
    "        new_action, log_prob, z, mean, log_std = self.policy_net.evaluate(state) #计算t时刻的动作、动作似然概率、正态分布抽样、分布均值和标准差\n",
    "\n",
    "\n",
    "        target_value = self.target_value_net(next_state) #计算t+1时刻的状态值\n",
    "        next_q_value = reward + (1 - done) * gamma * target_value # 时序差分计算t+1时刻的Q值\n",
    "        # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "        q_value_loss = self.soft_q_criterion(expected_q_value, next_q_value.detach()) #计算q网路的损失函数\n",
    "\n",
    "        expected_new_q_value = self.soft_q_net(state, new_action) #计算t时刻动作对应的q值\n",
    "        next_value = expected_new_q_value - log_prob # 计算t时刻的v值\n",
    "        value_loss = self.value_criterion(expected_value, next_value.detach()) #计算值网络损失函数\n",
    "        \n",
    "        ## 计算策略损失\n",
    "        log_prob_target = expected_new_q_value - expected_value \n",
    "        # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "        policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "\n",
    "        ## 计算reparameterization参数损失\n",
    "        mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "        std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "        z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "\n",
    "        policy_loss += mean_loss + std_loss + z_loss\n",
    "\n",
    "        self.soft_q_optimizer.zero_grad()\n",
    "        q_value_loss.backward()\n",
    "        self.soft_q_optimizer.step()\n",
    "\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        ## 更新目标值网络参数\n",
    "        for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2581771",
   "metadata": {},
   "source": [
    "## 2.模型训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3e3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    print(\"开始训练！\")\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        state = env.reset() # 重置环境，返回初始状态\n",
    "        ep_reward = 0 # 记录一回合内的奖励\n",
    "        for i_step in range(cfg.max_steps):\n",
    "            action = agent.policy_net.get_action(state)  # 抽样动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 更新环境，返回transitions\n",
    "            agent.memory.push(state, action, reward,next_state, terminated)  # 保存transition\n",
    "            agent.update()  # 更新智能体\n",
    "            state = next_state  # 更新下一个状态\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if terminated:\n",
    "                break\n",
    "        if (i_ep+1)%10 == 0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}\")\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"完成训练！\")\n",
    "    return {'rewards':rewards}\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        state = env.reset() # 重置环境，返回初始状态\n",
    "        ep_reward = 0\n",
    "        for i_step in range(cfg.max_steps):\n",
    "            action = agent.policy_net.get_action(state)  # 抽样动作\n",
    "            next_state, reward, terminated, info = env.step(action)  # 更新环境，返回transitions\n",
    "            state = next_state  # 更新下一个状态\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.2f}\")\n",
    "    print(\"完成测试！\")\n",
    "    return {'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d45832",
   "metadata": {},
   "source": [
    "## 3.定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b94efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        return action\n",
    "    \n",
    "def all_seed(env,seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    env.seed(seed) # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # config for CPU\n",
    "    torch.cuda.manual_seed(seed) # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "def env_agent_config(cfg):\n",
    "    env = NormalizedActions(gym.make(cfg.env_name)) # 创建环境\n",
    "    all_seed(env,seed=cfg.seed)\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    # 更新n_states和n_actions到cfg参数中\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    setattr(cfg, 'n_actions', n_actions) \n",
    "    setattr(cfg, 'action_space', env.action_space) \n",
    "    agent = SAC(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87423249",
   "metadata": {},
   "source": [
    "## 4.设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd710ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.algo_name = 'SAC'\n",
    "        self.env_name = 'Pendulum-v1'\n",
    "        self.seed = 50 # 随机种子\n",
    "        self.train_eps = 400 # 训练迭代次数\n",
    "        self.test_eps = 10 # 测试迭代次数\n",
    "        self.eval_eps = 10 # 评估迭代次数\n",
    "        self.max_steps = 200 # 每次迭代最大时间步\n",
    "        self.gamma = 0.99 #折扣因子\n",
    "        self.mean_lambda=1e-3 # 重参数化分布均值的损失权重\n",
    "        self.std_lambda=1e-3 # 重参数化分布标准差的损失权重\n",
    "        self.z_lambda=0.0 # 重参数化分布抽样值的损失权重\n",
    "        self.soft_tau=1e-2 # 目标网络软更新系数\n",
    "        self.value_lr  = 3e-4 # 值网络的学习率\n",
    "        self.soft_q_lr = 3e-4 # Q网络的学习率\n",
    "        self.policy_lr = 3e-4 # 策略网络的学习率\n",
    "        self.capacity = 1000000 # 经验缓存池的大小\n",
    "        self.hidden_dim = 256 # 隐藏层维度\n",
    "        self.batch_size  = 128 # 批次大小\n",
    "        self.start_steps = 1000 # 利用前的探索步数\n",
    "        self.buffer_size = 1000000 # 经验回放池大小\n",
    "        self.device=torch.device(\"cpu\") # 使用设备\n",
    "\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线，类似于Tensorboard中的smooth曲线\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,title=\"learning curve\"):\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlim(0, len(rewards), 10)  # 设置x轴的范围\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3f1c6",
   "metadata": {},
   "source": [
    "## 5.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a531f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80de3242",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PendulumEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m cfg = Config() \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m env, agent = \u001b[43menv_agent_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m res_dic = train(cfg, env, agent)\n\u001b[32m      7\u001b[39m plot_rewards(res_dic[\u001b[33m'\u001b[39m\u001b[33mrewards\u001b[39m\u001b[33m'\u001b[39m], title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining curve on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.algo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.env_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36menv_agent_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34menv_agent_config\u001b[39m(cfg):\n\u001b[32m     36\u001b[39m     env = NormalizedActions(gym.make(cfg.env_name)) \u001b[38;5;66;03m# 创建环境\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mall_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     n_states = env.observation_space.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     39\u001b[39m     n_actions = env.action_space.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mall_seed\u001b[39m\u001b[34m(env, seed)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mall_seed\u001b[39m(env,seed = \u001b[32m1\u001b[39m):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m''' 万能的seed函数\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m(seed) \u001b[38;5;66;03m# env config\u001b[39;00m\n\u001b[32m     26\u001b[39m     np.random.seed(seed)\n\u001b[32m     27\u001b[39m     random.seed(seed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:241\u001b[39m, in \u001b[36mWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maccessing private attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is prohibited\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:241\u001b[39m, in \u001b[36mWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maccessing private attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is prohibited\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Wrapper.__getattr__ at line 241 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\core.py:241\u001b[39m, in \u001b[36mWrapper.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maccessing private attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is prohibited\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PendulumEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = Config() \n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)\n",
    " \n",
    "plot_rewards(res_dic['rewards'], title=f\"training curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  \n",
    "# 测试\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], title=f\"testing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")  # 画出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46214798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badea21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
